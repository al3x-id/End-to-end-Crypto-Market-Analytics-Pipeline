{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cb5bac-88ed-4b65-b13e-d6f3af05009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "from requests import Session\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4baa7da-465e-4b83-a2b9-e3ca474ab2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data updated. Total rows: 5089\n"
     ]
    }
   ],
   "source": [
    "# Wait a few seconds to ensure network is ready (important for Task Scheduler)\n",
    "time.sleep(10)\n",
    "\n",
    "# Ignore irrelevant NumExpr warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"NumExpr defaulting to\")\n",
    "\n",
    "# Use a neutral, system-wide accessible directory\n",
    "BASE_DIR = r\"C:\\ETL\\crypto\"\n",
    "LOG_DIR = os.path.join(BASE_DIR, \"logs\")\n",
    "\n",
    "# Ensure folders exist\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Define consistent file paths\n",
    "RAW_FILE = os.path.join(BASE_DIR, \"crypto_raw.csv\")\n",
    "TRANSFORMED_FILE = os.path.join(BASE_DIR, \"crypto_transformed.csv\")\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"crypto_pipeline.log\")\n",
    "\n",
    "# Logging configuration\n",
    "if os.path.exists(LOG_FILE):\n",
    "    try:\n",
    "        os.remove(LOG_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not clear log file — {e}\")\n",
    "\n",
    "# Create a simple log file that refreshes each run\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    filemode='w'  # <-- this ensures overwrite mode\n",
    ")\n",
    "\n",
    "logging.info(\"ETL script started successfully\")\n",
    "logging.info(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "\n",
    "# Check write access before running\n",
    "try:\n",
    "    test_path = os.path.join(BASE_DIR, \"write_test.txt\")\n",
    "    with open(test_path, \"w\") as f:\n",
    "        f.write(\"Permission test OK\")\n",
    "    os.remove(test_path)\n",
    "    logging.info(\"Write permission test passed.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Write permission test failed: {e}\")\n",
    "    raise SystemExit(f\"Cannot write to {BASE_DIR}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# API configuration and extraction of data from CoinMarketCap\n",
    "def extract_data():\n",
    "    url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest'\n",
    "    parameters = {\n",
    "        'start': '1',\n",
    "        'limit': '5000',\n",
    "        'convert': 'USD'\n",
    "    }\n",
    "    headers = {\n",
    "        'Accepts': 'application/json',\n",
    "        'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY', 'API_KEY'),\n",
    "    }\n",
    "    \n",
    "    session = Session()\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Extracting data from CoinMarketCap API...\")\n",
    "        response = session.get(url, params=parameters)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "        # Create DataFrame from API response\n",
    "        df = pd.json_normalize(data['data'])\n",
    "\n",
    "        #Define a fixed raw file path\n",
    "        raw_file = RAW_FILE\n",
    "\n",
    "        #Check if raw file exist\n",
    "        if os.path.exists(raw_file):\n",
    "            #Read Existing data\n",
    "            old_df = pd.read_csv(raw_file)\n",
    "            #Combine new data with old\n",
    "            combined = pd.concat([old_df, df], ignore_index=True)\n",
    "            #Check and remove duplicates after combining data\n",
    "            combined.drop_duplicates(subset=['id'], keep='last', inplace=True)\n",
    "            #Save back to the raw file\n",
    "            combined.to_csv(raw_file, index=False)\n",
    "            logging.info(f\"Raw data updated. Total rows: {len(combined)}\")\n",
    "            print(f\"Raw data updated. Total rows: {len(combined)}\")\n",
    "        else:\n",
    "            #Create new raw file\n",
    "            df.to_csv(raw_file, index=False)\n",
    "            logging.info(f\"Raw data extracted and saved: {raw_file}\")\n",
    "            print(f\"Raw data extracted and saved: {raw_file}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except (ConnectionError, Timeout, TooManyRedirects) as e:\n",
    "        logging.error(f\"Error during extraction: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the function\n",
    "    result = extract_data()\n",
    "    if result is not None:\n",
    "        logging.info(f\"Extraction successful. Retrieved {len(result)} records.\")\n",
    "    else:\n",
    "        logging.error(\"Extraction failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcf7df9-0d25-48d2-979f-2f98f4ed7b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation completed. 5089 records processed.\n"
     ]
    }
   ],
   "source": [
    "#Transformation of extracted data (pulling only crucial columns and renaming)\n",
    "def transform_data(raw_df):\n",
    "    try:\n",
    "        logging.info(\"Starting data transformation...\")\n",
    "        df2 = raw_df.copy()\n",
    "        \n",
    "        # Convert datetime\n",
    "        df2[\"quote.USD.last_updated\"] = pd.to_datetime(df2[\"quote.USD.last_updated\"], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "        # Drop duplicate column\n",
    "        if 'last_updated' in df2.columns:\n",
    "            df2.drop(columns=['last_updated'], inplace=True)\n",
    "        \n",
    "        # Add category (handle NaN values)\n",
    "        df2[\"price_change_category\"] = df2[\"quote.USD.percent_change_24h\"].apply(\n",
    "            lambda x: \"Rise\" if pd.notna(x) and x > 0 else \"Drop\"\n",
    "        )\n",
    "        \n",
    "        # Rename important columns\n",
    "        df2.rename(columns={\n",
    "            \"quote.USD.price\": \"price\",\n",
    "            \"quote.USD.volume_24h\": \"volume_24h\",\n",
    "            \"quote.USD.percent_change_24h\": \"percent_change_24h\",\n",
    "            \"quote.USD.market_cap\": \"market_cap\",\n",
    "            \"quote.USD.last_updated\": \"last_updated\"\n",
    "        }, inplace=True)\n",
    "        \n",
    "        selected_cols = [\"id\", \"name\", \"symbol\", \"price\", \"volume_24h\",\n",
    "                         \"percent_change_24h\", \"market_cap\", \"last_updated\", \"price_change_category\"]\n",
    "        transformed = df2[selected_cols].copy()\n",
    "        \n",
    "        # Clean numeric columns\n",
    "        for col in [\"price\", \"volume_24h\", \"percent_change_24h\", \"market_cap\"]:\n",
    "            transformed[col] = pd.to_numeric(transformed[col], errors=\"coerce\")\n",
    "            transformed[col] = transformed[col].replace([np.inf, -np.inf], np.nan)\n",
    "            transformed[col] = transformed[col].fillna(0).round(8)\n",
    "        \n",
    "        # Sort and reset index\n",
    "        transformed = transformed.sort_values(by=\"id\", ascending=True).reset_index(drop=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        transformed_file = TRANSFORMED_FILE\n",
    "        os.makedirs(os.path.dirname(transformed_file), exist_ok=True)\n",
    "        transformed.to_csv(transformed_file, index=False)\n",
    "        \n",
    "        logging.info(f\"Transformation completed. File saved: {transformed_file}\")\n",
    "        print(f\"Transformation completed. {len(transformed)} records processed.\")\n",
    "        \n",
    "        return transformed\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Transformation failed: {e}\")\n",
    "        print(f\"Transformation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load raw data\n",
    "        raw_file = RAW_FILE\n",
    "        \n",
    "        if not os.path.exists(raw_file):\n",
    "            logging.error(\"Raw data file not found. Run extraction first.\")\n",
    "            print(\"Raw data file not found. Run extraction first.\")\n",
    "        else:\n",
    "            raw_df = pd.read_csv(raw_file)\n",
    "            result = transform_data(raw_df)\n",
    "            \n",
    "            if result is not None:\n",
    "                logging.info(f\"Transformation successful. Retrieved {len(result)} records.\")\n",
    "            else:\n",
    "                logging.error(\"Transformation failed.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {e}\")\n",
    "        print(f\"Error in main execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f507875f-da33-4669-9ff0-e8c681d38d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data successfully loaded to SQL Server!\n"
     ]
    }
   ],
   "source": [
    "#Loading the transformed data to SQL server\n",
    "def load_to_sql(transformed_file, server='ALEXANDER', database='CryptoDB', table_name='CryptoData'):\n",
    "    try:\n",
    "        logging.info(\"Starting SQL Server load process...\")\n",
    "        \n",
    "        # Validate input file\n",
    "        if not os.path.exists(transformed_file):\n",
    "            logging.error(f\"Transformed file not found: {transformed_file}\")\n",
    "            return False\n",
    "        \n",
    "        logging.info(f\"Reading transformed file: {transformed_file}\")\n",
    "        \n",
    "        # Build connection string for SQLAlchemy\n",
    "        conn_str = (\n",
    "            f\"mssql+pyodbc://@{server}/{database}\"\n",
    "            \"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "            \"&trusted_connection=yes\"\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Connecting to SQL Server: {server}/{database}\")\n",
    "        \n",
    "        # Create SQLAlchemy engine\n",
    "        engine = create_engine(conn_str, fast_executemany=True)\n",
    "        \n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(\"SELECT @@VERSION\"))\n",
    "            version = result.fetchone()[0][:50]\n",
    "            logging.info(f\"Connected successfully! SQL Server: {version}...\")\n",
    "        \n",
    "        # Step 1: Read transformed file\n",
    "        df3 = pd.read_csv(transformed_file)\n",
    "        logging.info(f\"Loaded {len(df3)} records from transformed file\")\n",
    "        \n",
    "        # Convert last_updated to datetime if needed\n",
    "        if 'last_updated' in df3.columns:\n",
    "            df3['last_updated'] = pd.to_datetime(df3['last_updated'], errors='coerce')\n",
    "        \n",
    "        # Step 2: Check if table exists and read existing IDs\n",
    "        try:\n",
    "            existing_ids = pd.read_sql(f\"SELECT id FROM {table_name}\", con=engine)\n",
    "            logging.info(f\"Found {len(existing_ids)} existing records in SQL table\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Table '{table_name}' might not exist yet. Will create it.\")\n",
    "            existing_ids = pd.DataFrame(columns=['id'])\n",
    "        \n",
    "        # Step 3: Filter out duplicates\n",
    "        if not existing_ids.empty:\n",
    "            df_filtered = df3[~df3[\"id\"].isin(existing_ids[\"id\"])]\n",
    "            duplicates_count = len(df3) - len(df_filtered)\n",
    "            if duplicates_count > 0:\n",
    "                logging.info(f\"Filtered out {duplicates_count} duplicate records\")\n",
    "        else:\n",
    "            df_filtered = df3\n",
    "            logging.info(\"No existing records - will insert all data\")\n",
    "        \n",
    "        logging.info(f\" {len(df_filtered)} new records ready for insert\")\n",
    "        \n",
    "        # Step 4: Insert only new ones\n",
    "        if not df_filtered.empty:\n",
    "            logging.info(f\"Inserting {len(df_filtered)} records into '{table_name}'...\")\n",
    "            df_filtered.to_sql(\"CryptoData\", con=engine, if_exists=\"append\", index=False)\n",
    "            \n",
    "            # Verify insertion\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(text(f\"SELECT COUNT(*) FROM {table_name}\"))\n",
    "                total_rows = result.fetchone()[0]\n",
    "                logging.info(f\"Successfully inserted {len(df_filtered)} new records\")\n",
    "                logging.info(f\" Total rows in '{table_name}': {total_rows}\")\n",
    "            \n",
    "            logging.info(\" SQL load completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            logging.info(\"No new data to insert — all records already exist\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during load: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    transformed_file = TRANSFORMED_FILE\n",
    "    \n",
    "    # SQL Server configuration\n",
    "    server = 'ALEXANDER'\n",
    "    database = 'CryptoDB'\n",
    "    table_name = 'CryptoData'\n",
    "    \n",
    "    # Load data\n",
    "    success = load_to_sql(\n",
    "        transformed_file=transformed_file,\n",
    "        server=server,\n",
    "        database=database,\n",
    "        table_name=table_name\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nData successfully loaded to SQL Server!\")\n",
    "    else:\n",
    "        print(\"\\nFailed to load data. Check logs for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
